#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import argparse
import torch
from typing import List, Dict, Any

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList

try:
    from duckduckgo_search import DDGS
    DUCK_AVAILABLE = True
except ImportError:
    DUCK_AVAILABLE = False
    DDGS = None


# ========================================
#  Crit√®re d'arr√™t pour la g√©n√©ration
# ========================================
class EOSStoppingCriteria(StoppingCriteria):
    """Arr√™te la g√©n√©ration quand [EOS] est produit."""
    def __init__(self, eos_token_id: int):
        self.eos_token_id = eos_token_id

    def __call__(self, input_ids, scores, **kwargs):
        # V√©rifie si les derniers tokens contiennent [EOS]
        last_token = input_ids[0, -1].item()
        return last_token == self.eos_token_id


# -----------------------------
#  Recherche web (DuckDuckGo)
# -----------------------------
def web_search(query: str, max_results: int = 3):
    """
    Retourne une liste de snippets textuels pour le RAG.
    Si DuckDuckGo n'est pas dispo, renvoie une liste vide.
    """
    if not DUCK_AVAILABLE:
        print("‚ö†Ô∏è duckduckgo_search non disponible, pas de recherche web.")
        return []

    results = []
    try:
        with DDGS(timeout=20) as ddgs:
            for r in ddgs.text(
                keywords=query,
                max_results=max_results,
                safesearch="moderate",
                region="wt-wt",
            ):
                results.append(r)
    except Exception as e:
        print(f"‚ùå Erreur DuckDuckGo pour la requ√™te '{query}': {e}")
        return []

    snippets = []
    for r in results:
        title = r.get("title") or ""
        body = r.get("body") or r.get("description") or ""
        txt = (title + " - " + body).strip()
        if txt:
            snippets.append(txt)

    return snippets


def format_evidences(snippets):
    """
    Formate les snippets fa√ßon 'R_Evidences'.
    """
    if not snippets:
        return "Title: dummy\nText: no evidence retrieved\n"

    docs = []
    for i, s in enumerate(snippets):
        docs.append(f"Title: doc{i}\nText: {s}")
    return "\n\n".join(docs)


# -----------------------------
#  Prompt agent autonome
# -----------------------------
def build_agent_prompt(question: str) -> str:
    """
    Prompt de d√©part : explique au mod√®le les tokens d'action.
    """
    system_msg = """You are an RQ-RAG agent.

You can use the following actions in your answer:
- [S_Rewritten_Query] ... [EOS]       to rewrite the question as a search query
- [S_Decomposed_Query] ... [EOS]      to decompose the question into simpler subquestions
- [S_Disambiguated_Query] ... [EOS]   to disambiguate an unclear question
- [A_Response] ... [EOS]              to give the final answer

You are NOT allowed to use [A_Response] before you have used at least one [S...] action.

Each time you use an [S_...] token, you MUST:
1) Write ONLY the query or sub-question text.
2) Then output [EOS].

After that, external tools may return evidence to you inside:
[R_Evidences] ... [/R_Evidences]

When you are ready to answer, use:
[A_Response] final answer here [EOS]
"""

    user_msg = f"Question: {question}"

    prompt = (
        "<s><|system|>\n" + system_msg + "\n</s>\n"
        "<|user|>\n" + user_msg + "\n</s>\n"
        "<|assistant|>\n"
    )
    return prompt


# ========================================
#  Agent autonome RQ-RAG
# ========================================
def rqrag_agent_autonome(
    model,
    tokenizer,
    question: str,
    max_steps: int = 4,
    max_new_tokens_step: int = 200,
    max_web_results: int = 3,
):
    """
    Impl√©mente un RQ-RAG 'agent autonome' :
    - le mod√®le d√©cide autonomously quand √©mettre [S_...] ou [A_Response]
    - chaque [S_...] d√©clenche une recherche web
    - affiche les √©tapes d√©taill√©es avant la r√©ponse finale
    """
    history_text = build_agent_prompt(question)
    actions_log = []
    seen_actions = set()
    full_conversation = []

    print("\n" + "="*50)
    print(f"ü§ñ QUESTION: {question}")
    print("="*50)

    for step in range(max_steps):
        print(f"\n{'‚îÄ'*50}")
        print(f"üìç √âTAPE {step+1}/{max_steps}")
        print(f"{'‚îÄ'*50}")

        # Tokenize l'historique
        inputs = tokenizer(
            history_text,
            return_tensors="pt",
            add_special_tokens=False,
        )
        input_ids = inputs["input_ids"].to(model.device)
        attention_mask = inputs["attention_mask"].to(model.device)

        print("üîÑ G√©n√©ration en cours...")

        # G√©n√©rer les tokens
        with torch.no_grad():
            output_ids = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens_step,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                num_beams=1,
            )

        # Extraire UNIQUEMENT les nouveaux tokens g√©n√©r√©s
        new_tokens = output_ids[0][input_ids.shape[1]:]
        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=False)
        
        # Afficher ce qui a √©t√© g√©n√©r√©
        print(f"\nüìù Texte g√©n√©r√©:\n{generated_text}")

        # Sauvegarder la g√©n√©ration compl√®te
        full_conversation.append(generated_text)

        # ====== 1) Chercher [A_Response] =====
        ans_matches = re.findall(
            r"\[A_Response\](.*?)\[EOS\]",
            generated_text,
            flags=re.DOTALL
        )
        
        if ans_matches:
            final_answer = ans_matches[-1].strip()
            
            # V√©rifier que le mod√®le a utilis√© au moins une action [S_...]
            if len(actions_log) == 0:
                print("\n‚ö†Ô∏è ALERTE: [A_Response] d√©tect√© SANS actions pr√©alables [S_...]")
                print("üîÅ Rejet de la r√©ponse. Le mod√®le doit d'abord faire une recherche/d√©composition.")
                
                # Ajouter un feedback au mod√®le
                history_text = (
                    history_text + generated_text +
                    "\n<|system|>\n‚ö†Ô∏è ERROR: You must use at least one [S_...] action before [A_Response]!\n"
                    "Please start by rewriting the question or decomposing it.\n</s>\n"
                    "<|assistant|>\n"
                )
                continue
            
            print(f"\n‚úÖ [A_Response] D√âTECT√â apr√®s {len(actions_log)} action(s)")
            print(f"üìå R√âPONSE FINALE: {final_answer}\n")
            
            return {
                "answer": final_answer,
                "full_conversation": "\n".join(full_conversation),
                "actions": actions_log,
                "stopped_by": "A_Response",
                "num_steps": step + 1,
            }

        # ====== 2) Chercher les actions [S_...] =====
        act_matches = re.findall(
            r"\[(S_Rewritten_Query|S_Decomposed_Query|S_Disambiguated_Query)\](.*?)\[EOS\]",
            generated_text,
            flags=re.DOTALL,
        )

        if act_matches:
            last_action, last_query = act_matches[-1]
            last_query = last_query.strip()

            # V√©rifier si on boucle
            if (last_action, last_query) in seen_actions:
                print(f"\n‚ö†Ô∏è BOUCLE D√âTECT√âE: M√™me action '{last_action}' avec m√™me query")
                print("‚ùå Arr√™t pour √©viter une boucle infinie.")
                break

            seen_actions.add((last_action, last_query))

            # Afficher l'action
            action_display = {
                "S_Rewritten_Query": "ÔøΩ R√â√âCRITURE DE REQU√äTE",
                "S_Decomposed_Query": "üîÄ D√âCOMPOSITION",
                "S_Disambiguated_Query": "‚ùì D√âSAMBIGU√èSATION",
            }
            
            print(f"\n{action_display.get(last_action, last_action)}")
            print(f"  ‚Üí Requ√™te: {last_query}")

            # Recherche web
            print("  üîç Recherche web...")
            snippets = web_search(last_query, max_results=max_web_results)
            
            if snippets:
                print(f"  ‚úì {len(snippets)} r√©sultat(s) trouv√©(s)")
            else:
                print("  ‚úó Aucun r√©sultat trouv√©")

            ev_text = format_evidences(snippets)

            actions_log.append({
                "action": last_action,
                "query": last_query,
                "snippets": snippets,
            })

            # R√©injecter les √©vidences pour la prochaine it√©ration
            history_text = (
                history_text + generated_text +
                "\n[R_Evidences]\n" +
                ev_text +
                "\n[/R_Evidences]\n" +
                "<|assistant|>\n"
            )
            continue

        # ====== 3) Rien d√©tect√© ‚Üí on arr√™te =====
        print("\n‚ö†Ô∏è Aucun token sp√©cial [S_...] ou [A_Response] d√©tect√©.")
        print("‚ùå Arr√™t de la g√©n√©ration.")
        break

    # Fin sans r√©ponse
    print("\n" + "="*50)
    print("‚ùå IMPOSSIBLE DE G√âN√âRER UNE R√âPONSE")
    print(f"(Arr√™t apr√®s {len(actions_log)} action(s), {step+1} √©tape(s))")
    print("="*50 + "\n")
    
    return {
        "answer": None,
        "full_conversation": "\n".join(full_conversation),
        "actions": actions_log,
        "stopped_by": "max_steps_or_no_action",
        "num_steps": step + 1,
    }


# ========================================
#  Chargement du mod√®le
# ========================================
def load_model_and_tokenizer():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("device =", device)

    model_name = os.environ.get("RQRAG_MODEL_NAME", "zorowin123/rq_rag_llama2_7B")
    hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN", None)

    # Charger tokenizer et mod√®le
    tokenizer = AutoTokenizer.from_pretrained(
        "../models/rq_rag_llama2_7B",
    )

    model = AutoModelForCausalLM.from_pretrained(
        "../models/rq_rag_llama2_7B",
        device_map="auto",
    )

    # Ajouter les tokens sp√©ciaux au vocabulaire
    special_tokens = {
        "additional_special_tokens": [
            "[S_Rewritten_Query]",
            "[S_Decomposed_Query]",
            "[S_Disambiguated_Query]",
            "[A_Response]",
            "[R_Evidences]",
            "[/R_Evidences]",
            "[EOS]",
        ]
    }
    tokenizer.add_special_tokens(special_tokens)
    model.resize_token_embeddings(len(tokenizer))

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    model.eval()
    print("‚úÖ Mod√®le charg√© :", model_name)
    print(f"‚úÖ Tokens sp√©ciaux ajout√©s. Vocabulaire: {len(tokenizer)} tokens")
    
    return model, tokenizer


# -----------------------------
#  Main
# -----------------------------
def parse_args():
    parser = argparse.ArgumentParser(
        description="Test 'agent autonome' pour RQ-RAG avec g√©n√©ration autonome des tokens sp√©ciaux."
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="Question unique √† tester.",
    )
    parser.add_argument(
        "--questions_file",
        type=str,
        default=None,
        help="Fichier texte avec une question par ligne.",
    )
    parser.add_argument(
        "--max_steps",
        type=int,
        default=4,
        help="Nombre max d'√©tapes/actions [S_...] avant d'arr√™ter.",
    )
    parser.add_argument(
        "--max_new_tokens_step",
        type=int,
        default=200,
        help="Nombre max de tokens g√©n√©r√©s par √©tape (augment√© pour laisser place aux tokens sp√©ciaux).",
    )
    parser.add_argument(
        "--max_web_results",
        type=int,
        default=3,
        help="Nombre max de r√©sultats web par requ√™te de recherche.",
    )
    return parser.parse_args()


def main():
    args = parse_args()

    if args.question is None and args.questions_file is None:
        print("‚ùå Erreur: Sp√©cifie soit --question, soit --questions_file.")
        return

    print("\n" + "="*60)
    print("üöÄ RQ-RAG AGENT AUTONOME - D√âMARRAGE")
    print("="*60 + "\n")

    model, tokenizer = load_model_and_tokenizer()

    questions = []
    if args.question is not None:
        questions.append(args.question)

    if args.questions_file is not None:
        with open(args.questions_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    questions.append(line)

    print(f"üìã {len(questions)} question(s) √† traiter\n")

    results_summary = []

    for idx, q in enumerate(questions, 1):
        print(f"\n{'#'*60}")
        print(f"# Question {idx}/{len(questions)}")
        print(f"{'#'*60}")
        
        res = rqrag_agent_autonome(
            model,
            tokenizer,
            q,
            max_steps=args.max_steps,
            max_new_tokens_step=args.max_new_tokens_step,
            max_web_results=args.max_web_results,
        )
        
        # Afficher le r√©capitulatif pour cette question
        print("\n" + "‚îÄ"*60)
        print("üìä R√âCAPITULATIF DE LA QUESTION")
        print("‚îÄ"*60)
        print(f"\n‚ùì Question: {q}\n")
        
        print(f"üìà Statut: {res['stopped_by']}")
        print(f"‚è±Ô∏è √âtapes effectu√©es: {res['num_steps']}/{args.max_steps}")
        print(f"üîé Actions ex√©cut√©es: {len(res['actions'])}")
        
        if res['actions']:
            print("\nüìç D√©tail des actions:")
            for i, action in enumerate(res['actions'], 1):
                action_name = action['action'].replace('S_', '').replace('_', ' ')
                print(f"  {i}. [{action_name}] {action['query']}")
                print(f"     ‚Üí {len(action['snippets'])} r√©sultat(s)")
        
        if res['answer']:
            print(f"\n‚úÖ R√âPONSE FINALE:")
            print(f"   {res['answer'][:200]}{'...' if len(res['answer']) > 200 else ''}\n")
        else:
            print(f"\n‚ùå PAS DE R√âPONSE G√âN√âR√âE\n")
        
        results_summary.append({
            'question': q,
            'answer': res['answer'],
            'num_actions': len(res['actions']),
            'stopped_by': res['stopped_by'],
        })

    # R√©sum√© final
    print("\n" + "="*60)
    print("üìã R√âSUM√â FINAL")
    print("="*60 + "\n")
    
    successful = sum(1 for r in results_summary if r['answer'] is not None)
    print(f"‚úÖ R√©ponses g√©n√©r√©es: {successful}/{len(results_summary)}")
    print(f"‚ùå √âchecs: {len(results_summary) - successful}/{len(results_summary)}")
    
    print("\n" + "="*60 + "\n")


if __name__ == "__main__":
    main()
